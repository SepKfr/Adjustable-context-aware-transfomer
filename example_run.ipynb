{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3QgWIS3hrP+C7x+hRX1Rb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SepKfr/Adjustable-context-aware-transfomer/blob/master/example_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SepKfr/Corruption-resilient-Forecasting-Models.git"
      ],
      "metadata": {
        "id": "D6QxdQA1V_Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Corruption-resilient-Forecasting-Models/"
      ],
      "metadata": {
        "id": "ePk50qvuWGyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''we run our models each time with three different \n",
        "random seeds generated by the bash file, therefore the results \n",
        "can be slightly different from one bash run to another, what is important\n",
        "is to show that the performance of our model is not based on luck and it can\n",
        "actucally perfrom better for the majority of runs with three \n",
        "different random seed each time'''\n",
        "\n",
        "!bash run.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYMnjlSdpWqy",
        "outputId": "7263984d-2b81-47cf-b9b4-8345a79b72c0"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 22:20:09,061]\u001b[0m A new study created in memory with name: ATA_gp\u001b[0m\n",
            "Train epoch: 0, loss: 124.6345\n",
            "val loss: 14.5438\n",
            "Train epoch: 5, loss: 102.1675\n",
            "val loss: 11.9817\n",
            "Train epoch: 10, loss: 79.0824\n",
            "val loss: 9.0511\n",
            "Train epoch: 15, loss: 71.3089\n",
            "val loss: 8.3995\n",
            "Train epoch: 20, loss: 65.9213\n",
            "val loss: 7.9061\n",
            "/usr/local/lib/python3.8/dist-packages/gpytorch/distributions/multivariate_normal.py:319: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-06.\n",
            "  warnings.warn(\n",
            "Train epoch: 25, loss: 61.2346\n",
            "val loss: 7.5152\n",
            "Train epoch: 30, loss: 57.6559\n",
            "val loss: 7.2874\n",
            "Train epoch: 35, loss: 54.7390\n",
            "val loss: 7.1164\n",
            "Train epoch: 40, loss: 52.9050\n",
            "val loss: 7.0363\n",
            "Train epoch: 45, loss: 51.0077\n",
            "val loss: 6.6284\n",
            "\u001b[32m[I 2023-01-30 22:26:13,714]\u001b[0m Trial 0 finished with value: 6.523269236087799 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.523269236087799.\u001b[0m\n",
            "\u001b[32m[I 2023-01-30 22:26:13,718]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 130.1344\n",
            "val loss: 14.9043\n",
            "Train epoch: 5, loss: 101.5578\n",
            "val loss: 11.8888\n",
            "Train epoch: 10, loss: 80.1064\n",
            "val loss: 10.9485\n",
            "Train epoch: 15, loss: 73.7002\n",
            "val loss: 9.0598\n",
            "Train epoch: 20, loss: 70.2616\n",
            "val loss: 8.2430\n",
            "Train epoch: 25, loss: 67.3733\n",
            "val loss: 8.1363\n",
            "Train epoch: 30, loss: 64.6190\n",
            "val loss: 7.8215\n",
            "Train epoch: 35, loss: 62.8582\n",
            "val loss: 7.8243\n",
            "Train epoch: 40, loss: 60.8965\n",
            "val loss: 7.4588\n",
            "Train epoch: 45, loss: 60.2120\n",
            "val loss: 7.3835\n",
            "\u001b[32m[I 2023-01-30 22:31:35,009]\u001b[0m Trial 2 finished with value: 7.1747225522994995 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.523269236087799.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  6.523269236087799\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.5303\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 22:32:55,132]\u001b[0m A new study created in memory with name: ATA_iso\u001b[0m\n",
            "Train epoch: 0, loss: 130.0025\n",
            "val loss: 14.8539\n",
            "Train epoch: 5, loss: 102.6096\n",
            "val loss: 12.6807\n",
            "Train epoch: 10, loss: 89.6489\n",
            "val loss: 10.6897\n",
            "Train epoch: 15, loss: 75.4009\n",
            "val loss: 8.7167\n",
            "Train epoch: 20, loss: 68.3223\n",
            "val loss: 8.0555\n",
            "/usr/local/lib/python3.8/dist-packages/gpytorch/distributions/multivariate_normal.py:319: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-06.\n",
            "  warnings.warn(\n",
            "Train epoch: 25, loss: 64.1758\n",
            "val loss: 7.7233\n",
            "Train epoch: 30, loss: 60.9945\n",
            "val loss: 7.4002\n",
            "Train epoch: 35, loss: 58.5863\n",
            "val loss: 7.7480\n",
            "Train epoch: 40, loss: 56.3226\n",
            "val loss: 7.2611\n",
            "Train epoch: 45, loss: 54.6235\n",
            "val loss: 6.9251\n",
            "\u001b[32m[I 2023-01-30 22:38:35,278]\u001b[0m Trial 0 finished with value: 6.872766315937042 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.872766315937042.\u001b[0m\n",
            "\u001b[32m[I 2023-01-30 22:38:35,283]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 136.5574\n",
            "val loss: 14.9858\n",
            "Train epoch: 5, loss: 102.7857\n",
            "val loss: 12.0973\n",
            "Train epoch: 10, loss: 82.1855\n",
            "val loss: 10.2313\n",
            "Train epoch: 15, loss: 76.3692\n",
            "val loss: 9.5774\n",
            "Train epoch: 20, loss: 71.7608\n",
            "val loss: 8.8545\n",
            "Train epoch: 25, loss: 68.4114\n",
            "val loss: 8.3250\n",
            "Train epoch: 30, loss: 65.9383\n",
            "val loss: 8.0414\n",
            "Train epoch: 35, loss: 64.0695\n",
            "val loss: 8.0816\n",
            "Train epoch: 40, loss: 62.7517\n",
            "val loss: 7.6342\n",
            "Train epoch: 45, loss: 60.9935\n",
            "val loss: 7.5369\n",
            "\u001b[32m[I 2023-01-30 22:43:48,707]\u001b[0m Trial 2 finished with value: 7.380939394235611 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.872766315937042.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  6.872766315937042\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.5587\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 22:45:09,173]\u001b[0m A new study created in memory with name: ATA_no\u001b[0m\n",
            "Train epoch: 0, loss: 129.5115\n",
            "val loss: 14.8025\n",
            "Train epoch: 5, loss: 110.1250\n",
            "val loss: 12.9482\n",
            "Train epoch: 10, loss: 88.1993\n",
            "val loss: 9.8879\n",
            "Train epoch: 15, loss: 77.4390\n",
            "val loss: 8.8853\n",
            "Train epoch: 20, loss: 71.1677\n",
            "val loss: 8.5790\n",
            "Train epoch: 25, loss: 67.5026\n",
            "val loss: 8.2633\n",
            "Train epoch: 30, loss: 65.2506\n",
            "val loss: 8.0876\n",
            "Train epoch: 35, loss: 63.1252\n",
            "val loss: 8.0946\n",
            "Train epoch: 40, loss: 61.5801\n",
            "val loss: 7.8372\n",
            "Train epoch: 45, loss: 60.3763\n",
            "val loss: 7.7166\n",
            "\u001b[32m[I 2023-01-30 22:50:07,256]\u001b[0m Trial 0 finished with value: 7.671910643577576 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 7.671910643577576.\u001b[0m\n",
            "\u001b[32m[I 2023-01-30 22:50:07,259]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 136.2604\n",
            "val loss: 14.9406\n",
            "Train epoch: 5, loss: 109.8606\n",
            "val loss: 12.9811\n",
            "Train epoch: 10, loss: 83.7822\n",
            "val loss: 10.0308\n",
            "Train epoch: 15, loss: 78.9630\n",
            "val loss: 9.6636\n",
            "Train epoch: 20, loss: 73.8739\n",
            "val loss: 8.9766\n",
            "Train epoch: 25, loss: 70.2281\n",
            "val loss: 8.3854\n",
            "Train epoch: 30, loss: 68.0895\n",
            "val loss: 8.3684\n",
            "Train epoch: 35, loss: 66.0952\n",
            "val loss: 8.2154\n",
            "Train epoch: 40, loss: 64.0385\n",
            "val loss: 8.0596\n",
            "Train epoch: 45, loss: 62.8386\n",
            "val loss: 7.8681\n",
            "\u001b[32m[I 2023-01-30 22:54:46,809]\u001b[0m Trial 2 finished with value: 7.82660984992981 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 7.671910643577576.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  7.671910643577576\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.6288\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 22:56:08,048]\u001b[0m A new study created in memory with name: ATA_gp\u001b[0m\n",
            "Train epoch: 0, loss: 128.3002\n",
            "val loss: 14.7192\n",
            "Train epoch: 5, loss: 103.6078\n",
            "val loss: 12.5703\n",
            "Train epoch: 10, loss: 81.9940\n",
            "val loss: 9.7657\n",
            "Train epoch: 15, loss: 72.4510\n",
            "val loss: 8.5570\n",
            "Train epoch: 20, loss: 67.4455\n",
            "val loss: 7.9873\n",
            "/usr/local/lib/python3.8/dist-packages/gpytorch/distributions/multivariate_normal.py:319: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-06.\n",
            "  warnings.warn(\n",
            "Train epoch: 25, loss: 63.3623\n",
            "val loss: 7.7395\n",
            "Train epoch: 30, loss: 59.8929\n",
            "val loss: 7.5416\n",
            "Train epoch: 35, loss: 57.6036\n",
            "val loss: 7.0835\n",
            "Train epoch: 40, loss: 55.3312\n",
            "val loss: 7.3504\n",
            "Train epoch: 45, loss: 52.7531\n",
            "val loss: 7.0210\n",
            "\u001b[32m[I 2023-01-30 23:01:56,936]\u001b[0m Trial 0 finished with value: 6.629759669303894 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.629759669303894.\u001b[0m\n",
            "\u001b[32m[I 2023-01-30 23:01:56,939]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 124.9100\n",
            "val loss: 14.7215\n",
            "Train epoch: 5, loss: 103.1171\n",
            "val loss: 12.9183\n",
            "Train epoch: 10, loss: 79.8858\n",
            "val loss: 9.2614\n",
            "Train epoch: 15, loss: 72.8238\n",
            "val loss: 8.5394\n",
            "Train epoch: 20, loss: 69.2266\n",
            "val loss: 8.5338\n",
            "Train epoch: 25, loss: 66.7309\n",
            "val loss: 7.9492\n",
            "Train epoch: 30, loss: 64.0995\n",
            "val loss: 7.6767\n",
            "Train epoch: 35, loss: 61.8355\n",
            "val loss: 7.3989\n",
            "Train epoch: 40, loss: 60.3558\n",
            "val loss: 7.2565\n",
            "Train epoch: 45, loss: 58.8696\n",
            "val loss: 7.1922\n",
            "\u001b[32m[I 2023-01-30 23:07:17,581]\u001b[0m Trial 2 finished with value: 6.93555012345314 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.629759669303894.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  6.629759669303894\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.5365\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 23:08:37,069]\u001b[0m A new study created in memory with name: ATA_iso\u001b[0m\n",
            "Train epoch: 0, loss: 126.4461\n",
            "val loss: 14.6857\n",
            "Train epoch: 5, loss: 104.5221\n",
            "val loss: 12.0765\n",
            "Train epoch: 10, loss: 82.6123\n",
            "val loss: 9.6824\n",
            "Train epoch: 15, loss: 73.5337\n",
            "val loss: 8.6593\n",
            "Train epoch: 20, loss: 67.6928\n",
            "val loss: 8.3134\n",
            "/usr/local/lib/python3.8/dist-packages/gpytorch/distributions/multivariate_normal.py:319: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-06.\n",
            "  warnings.warn(\n",
            "Train epoch: 25, loss: 62.9594\n",
            "val loss: 7.7942\n",
            "Train epoch: 30, loss: 59.8361\n",
            "val loss: 7.7605\n",
            "Train epoch: 35, loss: 57.9519\n",
            "val loss: 7.2486\n",
            "Train epoch: 40, loss: 56.4075\n",
            "val loss: 6.9619\n",
            "Train epoch: 45, loss: 54.8110\n",
            "val loss: 7.0783\n",
            "\u001b[32m[I 2023-01-30 23:14:21,449]\u001b[0m Trial 0 finished with value: 6.921832710504532 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.921832710504532.\u001b[0m\n",
            "\u001b[32m[I 2023-01-30 23:14:21,452]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 135.2704\n",
            "val loss: 14.9007\n",
            "Train epoch: 5, loss: 101.7697\n",
            "val loss: 10.7094\n",
            "Train epoch: 10, loss: 80.7347\n",
            "val loss: 9.7087\n",
            "Train epoch: 15, loss: 76.3181\n",
            "val loss: 8.7153\n",
            "Train epoch: 20, loss: 72.8620\n",
            "val loss: 8.5068\n",
            "Train epoch: 25, loss: 69.0951\n",
            "val loss: 8.4543\n",
            "Train epoch: 30, loss: 66.7187\n",
            "val loss: 8.0865\n",
            "Train epoch: 35, loss: 64.6549\n",
            "val loss: 7.6332\n",
            "Train epoch: 40, loss: 63.2092\n",
            "val loss: 7.6705\n",
            "Train epoch: 45, loss: 61.6572\n",
            "val loss: 7.5801\n",
            "\u001b[32m[I 2023-01-30 23:19:39,501]\u001b[0m Trial 2 finished with value: 7.338571399450302 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.921832710504532.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  6.921832710504532\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.5530\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 23:21:00,565]\u001b[0m A new study created in memory with name: ATA_no\u001b[0m\n",
            "Train epoch: 0, loss: 126.0139\n",
            "val loss: 14.6376\n",
            "Train epoch: 5, loss: 109.3196\n",
            "val loss: 13.0602\n",
            "Train epoch: 10, loss: 84.9297\n",
            "val loss: 10.0499\n",
            "Train epoch: 15, loss: 77.6180\n",
            "val loss: 9.1214\n",
            "Train epoch: 20, loss: 71.6870\n",
            "val loss: 8.5368\n",
            "Train epoch: 25, loss: 66.9810\n",
            "val loss: 8.1843\n",
            "Train epoch: 30, loss: 63.7671\n",
            "val loss: 7.8864\n",
            "Train epoch: 35, loss: 60.9362\n",
            "val loss: 7.6218\n",
            "Train epoch: 40, loss: 59.6806\n",
            "val loss: 7.5448\n",
            "Train epoch: 45, loss: 58.3001\n",
            "val loss: 7.5120\n",
            "\u001b[32m[I 2023-01-30 23:25:53,774]\u001b[0m Trial 0 finished with value: 7.468951940536499 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 7.468951940536499.\u001b[0m\n",
            "\u001b[32m[I 2023-01-30 23:25:53,776]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 134.9653\n",
            "val loss: 14.8477\n",
            "Train epoch: 5, loss: 109.5760\n",
            "val loss: 12.3946\n",
            "Train epoch: 10, loss: 83.5900\n",
            "val loss: 9.6194\n",
            "Train epoch: 15, loss: 78.7005\n",
            "val loss: 9.4027\n",
            "Train epoch: 20, loss: 74.4205\n",
            "val loss: 8.7252\n",
            "Train epoch: 25, loss: 70.7066\n",
            "val loss: 8.4545\n",
            "Train epoch: 30, loss: 68.4753\n",
            "val loss: 8.2835\n",
            "Train epoch: 35, loss: 66.6613\n",
            "val loss: 8.2530\n",
            "Train epoch: 40, loss: 65.2729\n",
            "val loss: 7.9728\n",
            "Train epoch: 45, loss: 64.1019\n",
            "val loss: 7.8825\n",
            "\u001b[32m[I 2023-01-30 23:30:29,282]\u001b[0m Trial 2 finished with value: 7.837698400020599 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 7.468951940536499.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  7.468951940536499\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.6052\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 23:31:47,682]\u001b[0m A new study created in memory with name: ATA_gp\u001b[0m\n",
            "Train epoch: 0, loss: 129.9956\n",
            "val loss: 14.6981\n",
            "Train epoch: 5, loss: 100.6166\n",
            "val loss: 12.6866\n",
            "Train epoch: 10, loss: 83.9716\n",
            "val loss: 10.1200\n",
            "Train epoch: 15, loss: 73.2401\n",
            "val loss: 9.1021\n",
            "Train epoch: 20, loss: 68.4725\n",
            "val loss: 8.6111\n",
            "/usr/local/lib/python3.8/dist-packages/gpytorch/distributions/multivariate_normal.py:319: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-06.\n",
            "  warnings.warn(\n",
            "Train epoch: 25, loss: 64.6817\n",
            "val loss: 8.0192\n",
            "Train epoch: 30, loss: 61.9638\n",
            "val loss: 7.8686\n",
            "Train epoch: 35, loss: 59.4411\n",
            "val loss: 7.5240\n",
            "Train epoch: 40, loss: 56.8363\n",
            "val loss: 7.3644\n",
            "Train epoch: 45, loss: 55.3518\n",
            "val loss: 6.9817\n",
            "\u001b[32m[I 2023-01-30 23:37:30,686]\u001b[0m Trial 0 finished with value: 6.836912423372269 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.836912423372269.\u001b[0m\n",
            "\u001b[32m[I 2023-01-30 23:37:30,688]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 130.3804\n",
            "val loss: 14.9149\n",
            "Train epoch: 5, loss: 98.2593\n",
            "val loss: 12.4433\n",
            "Train epoch: 10, loss: 81.4482\n",
            "val loss: 9.6811\n",
            "Train epoch: 15, loss: 75.5430\n",
            "val loss: 8.8660\n",
            "Train epoch: 20, loss: 71.4902\n",
            "val loss: 8.5062\n",
            "Train epoch: 25, loss: 68.3539\n",
            "val loss: 7.9618\n",
            "Train epoch: 30, loss: 65.8473\n",
            "val loss: 7.7512\n",
            "Train epoch: 35, loss: 64.2062\n",
            "val loss: 7.5823\n",
            "Train epoch: 40, loss: 61.8849\n",
            "val loss: 7.6806\n",
            "Train epoch: 45, loss: 60.5632\n",
            "val loss: 7.4870\n",
            "\u001b[32m[I 2023-01-30 23:42:47,113]\u001b[0m Trial 2 finished with value: 7.333299875259399 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.836912423372269.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  6.836912423372269\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.5606\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 23:44:04,991]\u001b[0m A new study created in memory with name: ATA_iso\u001b[0m\n",
            "Train epoch: 0, loss: 124.8410\n",
            "val loss: 14.6409\n",
            "Train epoch: 5, loss: 103.3431\n",
            "val loss: 12.2792\n",
            "Train epoch: 10, loss: 79.9408\n",
            "val loss: 9.4646\n",
            "Train epoch: 15, loss: 71.8458\n",
            "val loss: 8.5021\n",
            "Train epoch: 20, loss: 65.2230\n",
            "val loss: 8.6887\n",
            "/usr/local/lib/python3.8/dist-packages/gpytorch/distributions/multivariate_normal.py:319: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-06.\n",
            "  warnings.warn(\n",
            "Train epoch: 25, loss: 61.2708\n",
            "val loss: 7.8322\n",
            "Train epoch: 30, loss: 58.3382\n",
            "val loss: 7.3244\n",
            "Train epoch: 35, loss: 55.8691\n",
            "val loss: 6.9991\n",
            "Train epoch: 40, loss: 53.5549\n",
            "val loss: 6.9662\n",
            "Train epoch: 45, loss: 52.4776\n",
            "val loss: 6.8012\n",
            "\u001b[32m[I 2023-01-30 23:49:40,867]\u001b[0m Trial 0 finished with value: 6.801213264465332 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.801213264465332.\u001b[0m\n",
            "\u001b[32m[I 2023-01-30 23:49:40,870]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 126.6710\n",
            "val loss: 14.9975\n",
            "Train epoch: 5, loss: 103.3196\n",
            "val loss: 12.4227\n",
            "Train epoch: 10, loss: 81.6332\n",
            "val loss: 9.4134\n",
            "Train epoch: 15, loss: 74.8323\n",
            "val loss: 8.5244\n",
            "Train epoch: 20, loss: 70.7964\n",
            "val loss: 8.3585\n",
            "Train epoch: 25, loss: 66.7942\n",
            "val loss: 7.9115\n",
            "Train epoch: 30, loss: 64.0950\n",
            "val loss: 7.7859\n",
            "Train epoch: 35, loss: 62.5426\n",
            "val loss: 7.5444\n",
            "Train epoch: 40, loss: 61.0168\n",
            "val loss: 7.5430\n",
            "Train epoch: 45, loss: 59.7769\n",
            "val loss: 7.4888\n",
            "\u001b[32m[I 2023-01-30 23:54:51,548]\u001b[0m Trial 2 finished with value: 7.383773595094681 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 6.801213264465332.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  6.801213264465332\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.5750\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/data/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "\u001b[32m[I 2023-01-30 23:56:10,802]\u001b[0m A new study created in memory with name: ATA_no\u001b[0m\n",
            "Train epoch: 0, loss: 124.4557\n",
            "val loss: 14.6028\n",
            "Train epoch: 5, loss: 108.2088\n",
            "val loss: 12.5826\n",
            "Train epoch: 10, loss: 83.4648\n",
            "val loss: 10.4647\n",
            "Train epoch: 15, loss: 73.6248\n",
            "val loss: 10.5548\n",
            "Train epoch: 20, loss: 68.4145\n",
            "val loss: 8.1546\n",
            "Train epoch: 25, loss: 64.5905\n",
            "val loss: 8.3607\n",
            "Train epoch: 30, loss: 61.6969\n",
            "val loss: 7.8030\n",
            "Train epoch: 35, loss: 59.8878\n",
            "val loss: 7.8422\n",
            "Train epoch: 40, loss: 58.0829\n",
            "val loss: 7.4734\n",
            "Train epoch: 45, loss: 56.9803\n",
            "val loss: 7.3549\n",
            "\u001b[32m[I 2023-01-31 00:01:07,429]\u001b[0m Trial 0 finished with value: 7.354917287826538 and parameters: {'d_model': 32, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 7.354917287826538.\u001b[0m\n",
            "\u001b[32m[I 2023-01-31 00:01:07,432]\u001b[0m Trial 1 pruned. \u001b[0m\n",
            "Train epoch: 0, loss: 126.2972\n",
            "val loss: 14.9776\n",
            "Train epoch: 5, loss: 108.7736\n",
            "val loss: 12.7923\n",
            "Train epoch: 10, loss: 84.4714\n",
            "val loss: 9.6154\n",
            "Train epoch: 15, loss: 78.0113\n",
            "val loss: 9.0413\n",
            "Train epoch: 20, loss: 74.7725\n",
            "val loss: 8.6634\n",
            "Train epoch: 25, loss: 71.4334\n",
            "val loss: 8.5748\n",
            "Train epoch: 30, loss: 68.9872\n",
            "val loss: 8.6919\n",
            "Train epoch: 35, loss: 67.1773\n",
            "val loss: 8.4901\n",
            "Train epoch: 40, loss: 66.4333\n",
            "val loss: 8.2695\n",
            "Train epoch: 45, loss: 65.2820\n",
            "val loss: 8.3130\n",
            "\u001b[32m[I 2023-01-31 00:05:45,292]\u001b[0m Trial 2 finished with value: 8.230258136987686 and parameters: {'d_model': 16, 'w_steps': 1000, 'stack_size': 1}. Best is trial 0 with value: 7.354917287826538.\u001b[0m\n",
            "Study statistics: \n",
            "  Number of finished trials:  3\n",
            "  Number of pruned trials:  1\n",
            "  Number of complete trials:  2\n",
            "Best trial:\n",
            "  Value:  7.354917287826538\n",
            "  Params: \n",
            "    d_model: 32\n",
            "    w_steps: 1000\n",
            "    stack_size: 1\n",
            "test loss 0.6026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tb\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "from Utils.base_train import batching, ModelData, batch_sampled_data\n",
        "from data_loader import ExperimentConfig\n",
        "from forecast_denoising import Forecast_denoising\n",
        "\n",
        "exp_name = \"solar\"\n",
        "\n",
        "def evaluate(model_name, denoising, gp):\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    pred_len = 24\n",
        "    attn_type = \"ATA\"\n",
        "    n_heads = 8\n",
        "    d_model = [16, 32]\n",
        "    batch_size = 256\n",
        "\n",
        "    config = ExperimentConfig(pred_len, exp_name)\n",
        "\n",
        "    formatter = config.make_data_formatter()\n",
        "    params = formatter.get_experiment_params()\n",
        "    total_time_steps = params['total_time_steps']\n",
        "    num_encoder_steps = params['num_encoder_steps']\n",
        "    column_definition = params[\"column_definition\"]\n",
        "\n",
        "\n",
        "    data_csv_path = \"{}.csv\".format(exp_name)\n",
        "    raw_data = pd.read_csv(data_csv_path)\n",
        "\n",
        "    data = formatter.transform_data(raw_data)\n",
        "    train_max, valid_max = formatter.get_num_samples_for_calibration(num_train=batch_size)\n",
        "    max_samples = (train_max, valid_max)\n",
        "\n",
        "    _, _, test = batch_sampled_data(data, 0.8, max_samples, params['total_time_steps'],\n",
        "                                            params['num_encoder_steps'], pred_len,\n",
        "                                            params[\"column_definition\"],\n",
        "                                            batch_size)\n",
        "\n",
        "    test_enc, test_dec, test_y = next(iter(test))\n",
        "    total_b = len(list(iter(test)))\n",
        "\n",
        "\n",
        "    model_path = \"models_{}_{}\".format(exp_name, pred_len)\n",
        "    model_params = formatter.get_default_model_params()\n",
        "\n",
        "    src_input_size = test_enc.shape[2]\n",
        "    tgt_input_size = test_dec.shape[2]\n",
        "\n",
        "    predictions = np.zeros((3, total_b, test_y.shape[0], test_y.shape[1]))\n",
        "    test_y_tot = torch.zeros((total_b, test_y.shape[0], test_y.shape[1]))\n",
        "    n_batches_test = test_enc.shape[0]\n",
        "\n",
        "\n",
        "    mse = nn.MSELoss()\n",
        "    mae = nn.L1Loss()\n",
        "    stack_size = 1\n",
        "\n",
        "    seed_ls = []\n",
        "    i = -1\n",
        "    for f in os.listdir(model_path):\n",
        "      parts = f.split(\"_\")\n",
        "      \n",
        "      if model_name in f:\n",
        "        i += 1\n",
        "\n",
        "        seed = int(parts[-1])\n",
        "        for d in d_model:\n",
        "              try:\n",
        "                  \n",
        "                  d_k = int(d / n_heads)\n",
        "\n",
        "                  config = src_input_size, tgt_input_size, d, n_heads, d_k, stack_size\n",
        "\n",
        "                  model = Forecast_denoising(\n",
        "                                      pred_len=pred_len,\n",
        "                                      config=config,\n",
        "                                      model_name=model_name,\n",
        "                                      device=device,\n",
        "                                      attn_type=attn_type,\n",
        "                                      seed=seed,\n",
        "                                      denoise=denoising, gp=gp)\n",
        "\n",
        "                  checkpoint = torch.load(os.path.join(\"models_{}_{}\".format(exp_name, pred_len),\n",
        "                                          \"{}_{}\".format(model_name, seed)))\n",
        "                  state_dict = checkpoint['model_state_dict']\n",
        "                  new_state_dict = OrderedDict()\n",
        "\n",
        "                  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "                  model.eval()\n",
        "                  model.to(device)\n",
        "\n",
        "                  j = 0\n",
        "                  for test_enc, test_dec, test_y in test:\n",
        "                      if denoising:\n",
        "                          output, _ = model(test_enc.to(device), test_dec.to(device))\n",
        "                      else:\n",
        "                          output = model(test_enc.to(device), test_dec.to(device))\n",
        "\n",
        "                      predictions[i, j] = output.squeeze(-1).cpu().detach().numpy()\n",
        "                      if i == 0:\n",
        "                          test_y_tot[j] = test_y.squeeze(-1).cpu().detach()\n",
        "                      j += 1\n",
        "\n",
        "              except RuntimeError as e:\n",
        "                    pass\n",
        "        \n",
        "\n",
        "    predictions = torch.from_numpy(np.mean(predictions, axis=0))\n",
        "\n",
        "    results = torch.zeros(2, pred_len)\n",
        "    normaliser = test_y_tot.abs().mean()\n",
        "\n",
        "    mse_loss = mse(predictions, test_y_tot).item() / normaliser\n",
        "    mae_loss = mae(predictions, test_y_tot).item() / normaliser\n",
        "\n",
        "    print(\"{}: MSE: {:.3f}\".format(model_name, mse_loss))\n",
        "    print(\"{}: MAE: {:.3f}\".format(model_name, mae_loss))"
      ],
      "metadata": {
        "id": "TovjOwG1HFyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(\"ATA_gp\", True, True)\n",
        "evaluate(\"ATA_iso\", True, False)\n",
        "evaluate(\"ATA_no\", False, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-weC5WVIZEme",
        "outputId": "6a21d2dc-e679-48cc-f3cb-ec983784332c"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using root folder /content/Corruption-resilient-Forecasting-Models/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "ATA_gp: MSE: 0.493\n",
            "ATA_gp: MAE: 0.584\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "ATA_iso: MSE: 0.513\n",
            "ATA_iso: MAE: 0.591\n",
            "Using root folder /content/Corruption-resilient-Forecasting-Models/outputs\n",
            "Formatting data.\n",
            "Setting scalers with data_set...\n",
            "ATA_no: MSE: 0.570\n",
            "ATA_no: MAE: 0.636\n"
          ]
        }
      ]
    }
  ]
}